#!/usr/bin/env python3
"""
Improved Real-time Audio Emotion Recognition
- Exact preprocessing matching with training pipeline
- Voice Activity Detection (VAD)
- Silence/noise filtering
- Confidence calibration
- Robust error handling
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import pyaudio
import threading
import time
import json
import pickle
from collections import deque
from dataclasses import dataclass
import tkinter as tk
from tkinter import ttk, messagebox
import librosa
import queue
import logging
import sounddevice as sd
from scipy.signal import butter, filtfilt
import webrtcvad
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    import torch_directml
    device = torch_directml.device()
    print(f"Using DirectML device: {device}")
except ImportError:
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

@dataclass
class GUIUpdateData:
    emotion: str
    confidence: float
    tone_info: dict
    is_voice: bool
    energy: float
    spectral: float

class BalancedAudioCNN(nn.Module):
    def __init__(self, n_mfcc=40, n_frames=105, n_classes=8, dropout_rate=0.5):
        super(BalancedAudioCNN, self).__init__()
        
        self.n_mfcc = n_mfcc
        self.n_frames = n_frames
        self.n_classes = n_classes
        
        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.conv4 = nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1)
        self.bn4 = nn.BatchNorm2d(256)
        
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(dropout_rate)
        
        conv_output_size = (n_frames // 16) * (n_mfcc // 16) * 256
        
        self.fc1 = nn.Linear(conv_output_size, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, n_classes)
        
        self.layer_norm1 = nn.LayerNorm(512)
        self.layer_norm2 = nn.LayerNorm(256)
        self.layer_norm3 = nn.LayerNorm(128)
        
        self.relu = nn.ReLU()
        self.leaky_relu = nn.LeakyReLU(0.1)
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.pool(self.leaky_relu(self.bn1(self.conv1(x))))
        x = self.dropout(x)
        
        x = self.pool(self.leaky_relu(self.bn2(self.conv2(x))))
        x = self.dropout(x)
        
        x = self.pool(self.leaky_relu(self.bn3(self.conv3(x))))
        x = self.dropout(x)
        
        x = self.pool(self.leaky_relu(self.bn4(self.conv4(x))))
        x = self.dropout(x)
        
        x = x.view(x.size(0), -1)
        
        x = self.dropout(self.layer_norm1(self.leaky_relu(self.fc1(x))))
        x = self.dropout(self.layer_norm2(self.leaky_relu(self.fc2(x))))
        x = self.dropout(self.layer_norm3(self.leaky_relu(self.fc3(x))))
        x = self.fc4(x)
        
        return x

class RobustAudioCNN(nn.Module):
    def __init__(self, n_mfcc, n_classes, n_frames=None):
        super(RobustAudioCNN, self).__init__()
        
        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1)
        self.conv4 = nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1)
        
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.4)
        self.batch_norm1 = nn.BatchNorm2d(32)
        self.batch_norm2 = nn.BatchNorm2d(64)
        self.batch_norm3 = nn.BatchNorm2d(128)
        self.batch_norm4 = nn.BatchNorm2d(256)
        
        if n_frames is None:
            conv_output_size = (n_mfcc // 16) * 256
        else:
            conv_output_size = (n_frames // 16) * (n_mfcc // 16) * 256
        
        self.fc1 = nn.Linear(conv_output_size, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, n_classes)
        
        self.relu = nn.ReLU()
        self.leaky_relu = nn.LeakyReLU(0.1)
        
        self.layer_norm1 = nn.LayerNorm(512)
        self.layer_norm2 = nn.LayerNorm(256)
        self.layer_norm3 = nn.LayerNorm(128)
        
        self.n_mfcc = n_mfcc
        self.n_classes = n_classes
        self.n_frames = n_frames
    
    def forward(self, x):
        x1 = self.pool(self.leaky_relu(self.batch_norm1(self.conv1(x))))
        x2 = self.pool(self.leaky_relu(self.batch_norm2(self.conv2(x1))))
        x3 = self.pool(self.leaky_relu(self.batch_norm3(self.conv3(x2))))
        x4 = self.pool(self.leaky_relu(self.batch_norm4(self.conv4(x3))))
        
        x = x4.view(x4.size(0), -1)
        
        x = self.dropout(self.layer_norm1(self.leaky_relu(self.fc1(x))))
        x = self.dropout(self.layer_norm2(self.leaky_relu(self.fc2(x))))
        x = self.dropout(self.layer_norm3(self.leaky_relu(self.fc3(x))))
        x = self.fc4(x)
        
        return x

class VoiceActivityDetector:
    def __init__(self, sample_rate=16000, frame_duration=0.03, energy_threshold=0.01, spectral_threshold=0.1):
        self.sample_rate = sample_rate
        self.frame_duration = frame_duration
        self.frame_size = int(sample_rate * frame_duration)
        self.energy_threshold = energy_threshold
        self.spectral_threshold = spectral_threshold
        
    def detect_voice_activity(self, audio_data):
        if len(audio_data) < self.frame_size:
            return False, 0.0, 0.0
            
        energy = np.mean(audio_data**2)
        
        spectral_centroid = librosa.feature.spectral_centroid(
            y=audio_data, sr=self.sample_rate
        ).mean()
        
        spectral_centroid_norm = spectral_centroid / (self.sample_rate / 2)
        
        print(f"[VAD] Energy: {energy:.5f}, Spectral: {spectral_centroid_norm:.3f}, Thresholds: E>{self.energy_threshold}, S>{self.spectral_threshold}")
        
        has_voice = (energy > self.energy_threshold and 
                    spectral_centroid_norm > self.spectral_threshold)
        
        return has_voice, energy, spectral_centroid_norm

class ConfidenceCalibrator:
    def __init__(self, temperature=1.0, min_confidence=0.3):
        self.temperature = temperature
        self.min_confidence = min_confidence
        self.confidence_history = deque(maxlen=100)
        
    def calibrate_confidence(self, probabilities):
        scaled_probs = np.power(probabilities, 1/self.temperature)
        scaled_probs = scaled_probs / np.sum(scaled_probs)
        
        max_prob = np.max(scaled_probs)
        if max_prob < self.min_confidence:
            return scaled_probs, False
        
        return scaled_probs, True

class RobustEmotionModel(nn.Module):
    def __init__(self, num_classes=7, input_size=128):
        super(RobustEmotionModel, self).__init__()
        
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_size, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, num_classes)
        )
        
    def forward(self, x):
        features = self.feature_extractor(x)
        output = self.classifier(features)
        return output

class AudioConfig:
    def __init__(self):
        self.sample_rate = 16000
        self.chunk_size = 1024
        self.hop_length = 512
        self.n_mels = 128
        self.n_fft = 2048
        self.energy_threshold = 0.01
        self.spectral_threshold = 0.1

class EmotionStabilizer:
    def __init__(self, buffer_size=30, min_confidence=0.4, persistence_frames=15):
        self.emotion_buffer = []
        self.tone_buffer = []
        self.buffer_size = buffer_size
        self.min_confidence_threshold = min_confidence
        self.emotion_persistence_frames = persistence_frames
        self.last_stable_emotion = "neutral"
        self.last_stable_confidence = 0.0
        self.emotion_stability_counter = 0
    
    def add_emotion(self, emotion, confidence):
        self.emotion_buffer.append((emotion, confidence))
        if len(self.emotion_buffer) > self.buffer_size:
            self.emotion_buffer = self.emotion_buffer[-self.buffer_size:]
    
    def add_tone(self, tone_info):
        self.tone_buffer.append(tone_info)
    
    def get_stable_emotion(self):
        if not self.emotion_buffer:
            return self.last_stable_emotion, self.last_stable_confidence
        
        emotion_counts = {}
        total_confidence = 0
        
        for emotion, confidence in self.emotion_buffer:
            if emotion not in emotion_counts:
                emotion_counts[emotion] = {'count': 0, 'confidence': 0}
            emotion_counts[emotion]['count'] += 1
            emotion_counts[emotion]['confidence'] += confidence
            total_confidence += confidence
        
        if emotion_counts:
            sorted_emotions = sorted(emotion_counts.items(), 
                                   key=lambda x: (x[1]['count'], x[1]['confidence']/x[1]['count']), 
                                   reverse=True)
            
            most_frequent_emotion, stats = sorted_emotions[0]
            avg_confidence = stats['confidence'] / stats['count']
            
            min_samples = max(5, self.buffer_size // 6)
            
            if (stats['count'] >= min_samples and 
                avg_confidence >= self.min_confidence_threshold):
                
                if most_frequent_emotion != self.last_stable_emotion:
                    if stats['count'] >= len(self.emotion_buffer) * 0.4:
                        self.last_stable_emotion = most_frequent_emotion
                        self.last_stable_confidence = avg_confidence
                        self.emotion_stability_counter = 0
                    else:
                        self.emotion_stability_counter += 1
                else:
                    self.last_stable_confidence = avg_confidence
                    self.emotion_stability_counter = 0
            else:
                self.emotion_stability_counter += 1
        
        if self.emotion_stability_counter > self.emotion_persistence_frames:
            self.last_stable_emotion = "neutral"
            self.last_stable_confidence = 0.0
            self.emotion_stability_counter = 0
        
        return self.last_stable_emotion, self.last_stable_confidence

class ModelLoader:
    def __init__(self, device):
        self.device = device
        self.model = None
        self.config = None
        self.label_encoder = None
        self.emotion_labels = None
    
    def load_fixed_model(self):
        with open('models/fixed_model_config.json', 'r') as f:
            self.config = json.load(f)
        
        n_mfcc = self.config.get('n_mfcc', 40)
        n_frames = self.config.get('n_frames', 105)
        self.config['input_size'] = n_mfcc * n_frames + 15
        self.config['num_classes']